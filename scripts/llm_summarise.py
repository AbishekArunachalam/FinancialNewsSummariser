from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, BartForConditionalGeneration
from transformers import AutoTokenizer, logging, PegasusTokenizer, BartTokenizer
from transformers import GenerationConfig
from transformers import tokenization_utils_base


class SentimentAnalysis():
    pass

class SummariseNews():

    def __init__(self):
        
        logging.set_verbosity_error()
        self.eval_data = load_dataset("cnn_dailymail", "3.0.0", split="validation")
                                           
        self.t5_model_name='google/flan-t5-base'
        self.bart_model_name = "facebook/bart-large-cnn"
        self.pegasus_model_name = "google/pegasus-cnn_dailymail"

        self.auto_tokenizer = AutoTokenizer.from_pretrained(self.t5_model_name, use_fast=True)
        self.bart_tokenizer = BartTokenizer.from_pretrained(self.bart_model_name, use_fast=True)
        self.pegasus_tokenizer = PegasusTokenizer.from_pretrained(self.pegasus_model_name, use_fast=True)

        self.t5_model = AutoModelForSeq2SeqLM.from_pretrained(self.t5_model_name)
        self.bart_model = BartForConditionalGeneration.from_pretrained(self.bart_model_name)
        self.pegasus_model = PegasusForConditionalGeneration.from_pretrained(self.pegasus_model_name)


    def tokenise_text(self, news_article) -> tokenization_utils_base.BatchEncoding:
        """
        Args
        news_article: text of a financial news article

        Returns:
        tokenized_news: encoded pytorch tensors
        """
        tokenized_news = self.auto_tokenizer(news_article, return_tensors='pt')

        return tokenized_news

    def t5_summarise_news(self, token_txt) -> str:
        """
        Args
        token_txt: encoded tokens of pytorch tensors

        Returns:
        summary_txt: decoded summary of news article
        """

        summary_txt = self.auto_tokenizer.decode(
            self.t5_model.generate(
                token_txt["input_ids"], 
                max_new_tokens=40,
            )[0], 
            skip_special_tokens=True
        )

        return summary_txt
    
    def bart_summarise_news(self, token_txt) -> str:
        """
        Args
        token_txt: encoded tokens of pytorch tensors

        Returns:
        summary_txt: decoded summary of news article
        """

        summary_txt = self.bart_tokenizer.decode(
            self.bart_model.generate(
                token_txt["input_ids"], 
                max_new_tokens=60,
            )[0], 
            skip_special_tokens=True
        )

        return summary_txt

    def pegasus_summarise_news(self, token_txt) -> str:
        """
        Args
        token_txt: encoded tokens of pytorch tensors

        Returns:
        summary_txt: decoded summary of news article
        """

        summary_txt = self.pegasus_tokenizer.decode(
            self.pegasus_model.generate(
                token_txt["input_ids"], 
                max_new_tokens=60,
            )[0], 
            skip_special_tokens=True
        )
        return summary_txt

    def evaluate_model(self):
        print(self.eval_data)
        
        
#Alpaca API
# Financial BERT and BART model, Pegasus model, T5 (smaller model)


if __name__ == "__main__":
    obj = SummariseNews()
    with open("../data/news.txt", "r") as file:
        news = file.read()
        print(news)
    """
    token_txt = summarise.tokenise_text(news)
    print(f"Summary of new article generated by T5 model: {obj.t5_summarise_news(token_txt)}")
    print(f"Summary of new article generated by BART model: {obj.bart_summarise_news(token_txt)}")
    print(f"Summary of new article generated by Pegasus model: {obj.pegasus_summarise_news(token_txt)}")
    """
    obj.evaluate_model()